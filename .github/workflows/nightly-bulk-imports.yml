name: Nightly Bulk Data Import

on:
  # Run every night at 2:00 AM UTC
  schedule:
    - cron: '0 2 * * *'
  
  # Allow manual triggering for testing
  workflow_dispatch:
    inputs:
      debug_mode:
        description: 'Enable extra debug logging'
        required: false
        default: 'true'
        type: boolean

jobs:
  bulk-import:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Generous timeout to prevent crashes
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up environment
        run: |
          echo "üöÄ Starting nightly bulk data import"
          echo "üìÖ Run date: $(date -u)"
          echo "üîß GitHub Actions Runner: ubuntu-latest"
          echo "‚è∞ Timeout: 60 minutes"
      
      - name: Health Check - Ping Vercel Application
        id: health_check
        run: |
          echo "üè• Checking if Vercel application is reachable..."
          HEALTH_URL="${{ secrets.VERCEL_URL }}/api/health"
          
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" "$HEALTH_URL" || echo "000")
          
          if [ "$HTTP_CODE" = "200" ]; then
            echo "‚úÖ Vercel is healthy (HTTP $HTTP_CODE)"
          else
            echo "‚ö†Ô∏è Vercel returned HTTP $HTTP_CODE"
            echo "‚ö†Ô∏è Continuing anyway"
          fi
          
          echo "health_status=$HTTP_CODE" >> $GITHUB_OUTPUT
      
      - name: Wake Up Render Service (FREE tier spindown handling)
        id: render_wakeup
        run: |
          echo "üåÖ Waking up Render FREE tier service..."
          echo "üí§ Render spins down after 15min inactivity"
          echo "‚è∞ Cold start takes ~60-90 seconds"
          
          RENDER_HEALTH="https://mtg-bulk-jobs.onrender.com/health"
          
          echo "üîî Pinging Render health endpoint (2 min timeout)..."
          START=$(date +%s)
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --max-time 120 "$RENDER_HEALTH" || echo "000")
          END=$(date +%s)
          DURATION=$((END - START))
          
          echo "üì° Response: HTTP $HTTP_CODE in ${DURATION}s"
          
          if [ "$HTTP_CODE" = "200" ]; then
            echo "‚úÖ Render is awake!"
            if [ $DURATION -gt 10 ]; then
              echo "‚è∞ Service was cold, took ${DURATION}s to wake up"
              echo "‚è≥ Waiting 20 more seconds for full initialization..."
              sleep 20
            else
              echo "üöÄ Service was already warm!"
            fi
          else
            echo "‚ùå Render health check failed (HTTP $HTTP_CODE)"
            echo "‚è≥ Waiting 60s and retrying..."
            sleep 60
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --max-time 120 "$RENDER_HEALTH" || echo "000")
            if [ "$HTTP_CODE" = "200" ]; then
              echo "‚úÖ Render is awake after retry!"
              echo "‚è≥ Waiting 20s for full initialization..."
              sleep 20
            else
              echo "‚ùå Render still not responding, jobs will likely fail"
              exit 1
            fi
          fi
      
      # Job 1 - DISABLED: Run locally due to Render FREE tier 512MB RAM limit
      # Bulk Scryfall import requires ~100MB+ JSON file which exceeds free tier
      # To run manually: cd bulk-jobs-server && npm start, then POST to localhost:3001/bulk-scryfall
      
      # - name: Job 1 - Bulk Scryfall Import (Metadata)
      #   id: job1
      #   continue-on-error: false
      #   run: |
      #     echo "üé® =========================================="
      #     echo "üé® JOB 1: Bulk Scryfall Import (Metadata)"
      #     echo "üé® =========================================="
      #     echo "üìã What: Downloads ALL 110k+ cards with metadata"
      #     echo "üíæ Target: scryfall_cache table"
      #     echo "‚è±Ô∏è Expected: 3-5 minutes"
      #     echo ""
      #     
      #     START_TIME=$(date +%s)
      #     echo "üïê Start time: $(date -u)"
      #     
      #     API_URL="https://mtg-bulk-jobs.onrender.com/bulk-scryfall"
      #     echo "üåê Calling: $API_URL"
      #     
      #     # Make the request with detailed output
      #     HTTP_CODE=$(curl -X POST \
      #       -H "x-cron-key: ${{ secrets.CRON_KEY }}" \
      #       -H "Content-Type: application/json" \
      #       -H "User-Agent: GitHub-Actions-Nightly-Import/1.0" \
      #       -w "\n%{http_code}" \
      #       -s \
      #       --max-time 600 \
      #       --connect-timeout 30 \
      #       "$API_URL" \
      #       -o response.json)
      #     
      #     END_TIME=$(date +%s)
      #     DURATION=$((END_TIME - START_TIME))
      #     
      #     echo ""
      #     echo "‚è±Ô∏è Completed in: ${DURATION} seconds"
      #     echo "üì° HTTP Status: $HTTP_CODE"
      #     echo ""
      #     echo "üìÑ Response body:"
      #     cat response.json | jq '.' || cat response.json
      #     echo ""
      #     
      #     # Check for success
      #     if [ "$HTTP_CODE" = "200" ]; then
      #       echo "‚úÖ Job 1 completed successfully!"
      #       
      #       # Extract stats if available
      #       INSERTED=$(cat response.json | jq -r '.inserted // "N/A"')
      #       PROCESSED=$(cat response.json | jq -r '.processed // "N/A"')
      #       echo "üìä Stats: Processed=$PROCESSED, Inserted=$INSERTED"
      #     else
      #       echo "‚ùå Job 1 failed with HTTP $HTTP_CODE"
      #       echo "üîç Error details:"
      #       cat response.json | jq -r '.error // "No error message"' || echo "Could not parse error"
      #       exit 1
      #     fi
      #     
      #     echo "job1_duration=$DURATION" >> $GITHUB_OUTPUT
      #     echo "job1_status=success" >> $GITHUB_OUTPUT
      
      # Wait removed - Job 1 disabled
      
      - name: Job 2 - Bulk Price Import (Live Prices)
        id: job2
        continue-on-error: false
        run: |
          echo "üí∞ =========================================="
          echo "üí∞ JOB 2: Bulk Price Import (Live Prices)"
          echo "üí∞ =========================================="
          echo "üìã What: Updates prices for ALL cached cards"
          echo "üíæ Target: scryfall_cache table (price fields)"
          echo "‚è±Ô∏è Expected: 3-5 minutes"
          echo ""
          
          START_TIME=$(date +%s)
          echo "üïê Start time: $(date -u)"
          
          API_URL="https://mtg-bulk-jobs.onrender.com/bulk-price-import"
          echo "üåê Calling: $API_URL"
          
          HTTP_CODE=$(curl -X POST \
            -H "x-cron-key: ${{ secrets.CRON_KEY }}" \
            -H "Content-Type: application/json" \
            -H "User-Agent: GitHub-Actions-Nightly-Import/1.0" \
            -w "%{http_code}" \
            -s \
            --max-time 600 \
            --connect-timeout 30 \
            "$API_URL" \
            -o response.json)
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          echo ""
          echo "‚è±Ô∏è Completed in: ${DURATION} seconds"
          echo "üì° HTTP Status: $HTTP_CODE"
          echo ""
          echo "üìÑ Response body:"
          cat response.json | jq '.' || cat response.json
          echo ""
          
          if [ "$HTTP_CODE" = "200" ] || [ "$HTTP_CODE" = "202" ]; then
            echo "‚úÖ Job 2 completed successfully! (HTTP $HTTP_CODE)"
            
            if [ "$HTTP_CODE" = "202" ]; then
              echo "‚ÑπÔ∏è Job accepted and running in background (3-5 minutes)"
            fi
            
            UPDATED=$(cat response.json | jq -r '.updated // "N/A"')
            COVERAGE=$(cat response.json | jq -r '.coverage // "N/A"')
            echo "üìä Stats: Updated=$UPDATED, Coverage=$COVERAGE"
          else
            echo "‚ùå Job 2 failed with HTTP $HTTP_CODE"
            echo "üîç Error details:"
            cat response.json | jq -r '.error // "No error message"' || echo "Could not parse error"
            exit 1
          fi
          
          echo "job2_duration=$DURATION" >> $GITHUB_OUTPUT
          echo "job2_status=success" >> $GITHUB_OUTPUT
      
      - name: Verify Job 2 Completion
        id: verify_job2
        continue-on-error: true
        run: |
          echo ""
          echo "üîç =========================================="
          echo "üîç VERIFYING JOB 2 COMPLETION"
          echo "üîç =========================================="
          echo ""
          
          # Query Supabase to check if job completed
          SUPABASE_URL="${{ secrets.SUPABASE_URL }}"
          SUPABASE_KEY="${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}"
          
          # Validate secrets are set
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "‚ùå Missing Supabase secrets (SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY)"
            echo "verify_job2_status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Sanitize SUPABASE_URL to remove hidden control characters and trailing slashes
          SUPABASE_URL_CLEAN=$(printf %s "$SUPABASE_URL" | tr -d '\r\n ' | sed -E 's#/*$##')
          
          echo "üîç Querying Supabase..."
          echo "üîç Original URL length: ${#SUPABASE_URL} characters"
          echo "üîç Cleaned URL length: ${#SUPABASE_URL_CLEAN} characters"
          echo "üîç URL prefix: ${SUPABASE_URL_CLEAN%%/*}..." # Show domain only for security
          echo "üîç Key length: ${#SUPABASE_KEY} characters"
          echo ""
          
          # Check for timestamp within last 30 minutes (more forgiving, accounts for job duration)
          CURRENT_EPOCH=$(date -u +%s)
          CUTOFF_EPOCH=$((CURRENT_EPOCH - 1800))  # 30 minutes ago
          
          echo "‚è∞ Current time: $(date -u -d "@$CURRENT_EPOCH" +"%Y-%m-%dT%H:%M:%S" 2>/dev/null || date -u)"
          echo "‚è∞ Checking for timestamp after: $(date -u -d "@$CUTOFF_EPOCH" +"%Y-%m-%dT%H:%M:%S" 2>/dev/null || date -u)"
          echo "   (looking for timestamps within last 30 minutes)"
          echo ""
          
          # Poll for up to 12 minutes (24 attempts, every 30 seconds)
          MAX_ATTEMPTS=24
          ATTEMPT=1
          FOUND=false
          
          echo "‚è≥ Polling database every 30s for job completion (max ${MAX_ATTEMPTS} attempts = 12 minutes)..."
          
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo ""
            echo "üîç Attempt $ATTEMPT/$MAX_ATTEMPTS: $(date -u)"
            
            # Query Supabase for last run timestamp
            RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" \
              -H "apikey: $SUPABASE_KEY" \
              -H "Authorization: Bearer $SUPABASE_KEY" \
              --max-time 15 \
              --connect-timeout 10 \
              --retry 2 \
              --retry-delay 2 \
              "$SUPABASE_URL_CLEAN/rest/v1/app_config?key=eq.job:last:bulk_price_import&select=value" 2>&1)
            
            CURL_EXIT=$?
            HTTP_CODE=$(echo "$RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2 || echo "000")
            RESPONSE_BODY=$(echo "$RESPONSE" | grep -v "HTTP_CODE:" | grep -v "^[<>]" | grep -v "^[{}]" | grep -v "^[*]" | tail -1)
            
            if [ "$CURL_EXIT" != "0" ] || [ "$HTTP_CODE" != "200" ]; then
              echo "‚ö†Ô∏è Query failed (HTTP $HTTP_CODE, curl exit $CURL_EXIT) - retrying in 30s..."
              sleep 30
              ATTEMPT=$((ATTEMPT + 1))
              continue
            fi
            
            LAST_RUN=$(echo "$RESPONSE_BODY" | jq -r '.[0].value // empty' 2>/dev/null || echo "")
            
            if [ -z "$LAST_RUN" ] || [ "$LAST_RUN" = "null" ]; then
              echo "‚ö†Ô∏è No timestamp found yet - retrying in 30s..."
              sleep 30
              ATTEMPT=$((ATTEMPT + 1))
              continue
            fi
            
            # Parse timestamp to epoch
            LAST_RUN_CLEANED=$(echo "$LAST_RUN" | sed 's/\.[0-9]*//' | sed 's/Z$//')
            LAST_RUN_EPOCH=$(date -u -d "$LAST_RUN_CLEANED" +%s 2>/dev/null || date -u -j -f "%Y-%m-%dT%H:%M:%S" "$LAST_RUN_CLEANED" +%s 2>/dev/null || echo "0")
            
            echo "üìÖ Last run timestamp: $LAST_RUN"
            echo "üî¢ Timestamp epoch: $LAST_RUN_EPOCH"
            echo "üî¢ Cutoff epoch: $CUTOFF_EPOCH"
            
            if [ "$LAST_RUN_EPOCH" != "0" ] && [ $LAST_RUN_EPOCH -gt $CUTOFF_EPOCH ]; then
              echo ""
              echo "‚úÖ Job 2 verified - timestamp updated successfully!"
              echo "üìÖ Timestamp: $LAST_RUN (was updated after job start)"
              echo "verify_job2_status=success" >> $GITHUB_OUTPUT
              FOUND=true
              break
            else
              echo "‚è≥ Timestamp not recent enough yet (or parsing failed) - retrying in 30s..."
              sleep 30
              ATTEMPT=$((ATTEMPT + 1))
            fi
          done
          
          if [ "$FOUND" = "false" ]; then
            echo ""
            echo "‚ö†Ô∏è Job 2 verification inconclusive - timestamp not updated within 12 minutes"
            echo "‚ö†Ô∏è This could mean the job is still running, failed silently, or took longer than expected"
            echo "‚ö†Ô∏è Continuing workflow anyway..."
            echo "verify_job2_status=inconclusive" >> $GITHUB_OUTPUT
            exit 0  # Don't fail the workflow, just mark as inconclusive
          fi
      
      - name: Wait between jobs (5 seconds)
        run: |
          echo "‚è∏Ô∏è Waiting 5 seconds before next job..."
          sleep 5
      
      - name: Job 3 - Historical Price Snapshots
        id: job3
        continue-on-error: false
        run: |
          echo "üìà =========================================="
          echo "üìà JOB 3: Historical Price Snapshots"
          echo "üìà =========================================="
          echo "üìã What: Creates historical price snapshots"
          echo "üíæ Target: price_snapshots table"
          echo "‚è±Ô∏è Expected: 2-3 minutes"
          echo ""
          
          START_TIME=$(date +%s)
          echo "üïê Start time: $(date -u)"
          
          API_URL="https://mtg-bulk-jobs.onrender.com/price-snapshot"
          echo "üåê Calling: $API_URL"
          
          HTTP_CODE=$(curl -X POST \
            -H "x-cron-key: ${{ secrets.CRON_KEY }}" \
            -H "Content-Type: application/json" \
            -H "User-Agent: GitHub-Actions-Nightly-Import/1.0" \
            -w "%{http_code}" \
            -s \
            --max-time 600 \
            --connect-timeout 30 \
            "$API_URL" \
            -o response.json)
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          echo ""
          echo "‚è±Ô∏è Completed in: ${DURATION} seconds"
          echo "üì° HTTP Status: $HTTP_CODE"
          echo ""
          echo "üìÑ Response body:"
          cat response.json | jq '.' || cat response.json
          echo ""
          
          if [ "$HTTP_CODE" = "200" ] || [ "$HTTP_CODE" = "202" ]; then
            echo "‚úÖ Job 3 completed successfully! (HTTP $HTTP_CODE)"
            
            if [ "$HTTP_CODE" = "202" ]; then
              echo "‚ÑπÔ∏è Job accepted and running in background (2-3 minutes)"
            fi
            
            INSERTED=$(cat response.json | jq -r '.inserted // "N/A"')
            SNAPSHOT_DATE=$(cat response.json | jq -r '.snapshot_date // "N/A"')
            echo "üìä Stats: Inserted=$INSERTED, Date=$SNAPSHOT_DATE"
          else
            echo "‚ùå Job 3 failed with HTTP $HTTP_CODE"
            echo "üîç Error details:"
            cat response.json | jq -r '.error // "No error message"' || echo "Could not parse error"
            exit 1
          fi
          
          echo "job3_duration=$DURATION" >> $GITHUB_OUTPUT
          echo "job3_status=success" >> $GITHUB_OUTPUT
      
      - name: Verify Job 3 Completion
        id: verify_job3
        continue-on-error: true
        run: |
          echo ""
          echo "üîç =========================================="
          echo "üîç VERIFYING JOB 3 COMPLETION"
          echo "üîç =========================================="
          echo ""
          
          # Query Supabase to check if job completed
          SUPABASE_URL="${{ secrets.SUPABASE_URL }}"
          SUPABASE_KEY="${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}"
          
          # Validate secrets are set
          if [ -z "$SUPABASE_URL" ] || [ -z "$SUPABASE_KEY" ]; then
            echo "‚ùå Missing Supabase secrets (SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY)"
            echo "verify_job3_status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Sanitize SUPABASE_URL to remove hidden control characters and trailing slashes
          SUPABASE_URL_CLEAN=$(printf %s "$SUPABASE_URL" | tr -d '\r\n ' | sed -E 's#/*$##')
          
          echo "üîç Querying Supabase..."
          echo "üîç Original URL length: ${#SUPABASE_URL} characters"
          echo "üîç Cleaned URL length: ${#SUPABASE_URL_CLEAN} characters"
          echo "üîç URL prefix: ${SUPABASE_URL_CLEAN%%/*}..." # Show domain only for security
          echo "üîç Key length: ${#SUPABASE_KEY} characters"
          echo ""
          
          # Check for timestamp within last 30 minutes (more forgiving, accounts for job duration)
          CURRENT_EPOCH=$(date -u +%s)
          CUTOFF_EPOCH=$((CURRENT_EPOCH - 1800))  # 30 minutes ago
          
          echo "‚è∞ Current time: $(date -u -d "@$CURRENT_EPOCH" +"%Y-%m-%dT%H:%M:%S" 2>/dev/null || date -u)"
          echo "‚è∞ Checking for timestamp after: $(date -u -d "@$CUTOFF_EPOCH" +"%Y-%m-%dT%H:%M:%S" 2>/dev/null || date -u)"
          echo "   (looking for timestamps within last 30 minutes)"
          echo ""
          
          # Poll for up to 10 minutes (20 attempts, every 30 seconds)
          MAX_ATTEMPTS=20
          ATTEMPT=1
          FOUND=false
          
          echo "‚è≥ Polling database every 30s for job completion (max ${MAX_ATTEMPTS} attempts = 10 minutes)..."
          
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            echo ""
            echo "üîç Attempt $ATTEMPT/$MAX_ATTEMPTS: $(date -u)"
            
            # Query Supabase for last run timestamp
            RESPONSE=$(curl -s -w "\nHTTP_CODE:%{http_code}" \
              -H "apikey: $SUPABASE_KEY" \
              -H "Authorization: Bearer $SUPABASE_KEY" \
              --max-time 15 \
              --connect-timeout 10 \
              --retry 2 \
              --retry-delay 2 \
              "$SUPABASE_URL_CLEAN/rest/v1/app_config?key=eq.job:last:price_snapshot_bulk&select=value" 2>&1)
            
            CURL_EXIT=$?
            HTTP_CODE=$(echo "$RESPONSE" | grep "HTTP_CODE:" | cut -d: -f2 || echo "000")
            RESPONSE_BODY=$(echo "$RESPONSE" | grep -v "HTTP_CODE:" | grep -v "^[<>]" | grep -v "^[{}]" | grep -v "^[*]" | tail -1)
            
            if [ "$CURL_EXIT" != "0" ] || [ "$HTTP_CODE" != "200" ]; then
              echo "‚ö†Ô∏è Query failed (HTTP $HTTP_CODE, curl exit $CURL_EXIT) - retrying in 30s..."
              sleep 30
              ATTEMPT=$((ATTEMPT + 1))
              continue
            fi
            
            LAST_RUN=$(echo "$RESPONSE_BODY" | jq -r '.[0].value // empty' 2>/dev/null || echo "")
            
            if [ -z "$LAST_RUN" ] || [ "$LAST_RUN" = "null" ]; then
              echo "‚ö†Ô∏è No timestamp found yet - retrying in 30s..."
              sleep 30
              ATTEMPT=$((ATTEMPT + 1))
              continue
            fi
            
            # Parse timestamp to epoch
            LAST_RUN_CLEANED=$(echo "$LAST_RUN" | sed 's/\.[0-9]*//' | sed 's/Z$//')
            LAST_RUN_EPOCH=$(date -u -d "$LAST_RUN_CLEANED" +%s 2>/dev/null || date -u -j -f "%Y-%m-%dT%H:%M:%S" "$LAST_RUN_CLEANED" +%s 2>/dev/null || echo "0")
            
            echo "üìÖ Last run timestamp: $LAST_RUN"
            echo "üî¢ Timestamp epoch: $LAST_RUN_EPOCH"
            echo "üî¢ Cutoff epoch: $CUTOFF_EPOCH"
            
            if [ "$LAST_RUN_EPOCH" != "0" ] && [ $LAST_RUN_EPOCH -gt $CUTOFF_EPOCH ]; then
              echo ""
              echo "‚úÖ Job 3 verified - timestamp updated successfully!"
              echo "üìÖ Timestamp: $LAST_RUN (was updated after job start)"
              echo "verify_job3_status=success" >> $GITHUB_OUTPUT
              FOUND=true
              break
            else
              echo "‚è≥ Timestamp not recent enough yet (or parsing failed) - retrying in 30s..."
              sleep 30
              ATTEMPT=$((ATTEMPT + 1))
            fi
          done
          
          if [ "$FOUND" = "false" ]; then
            echo ""
            echo "‚ö†Ô∏è Job 3 verification inconclusive - timestamp not updated within 10 minutes"
            echo "‚ö†Ô∏è This could mean the job is still running, failed silently, or took longer than expected"
            echo "‚ö†Ô∏è Continuing workflow anyway..."
            echo "verify_job3_status=inconclusive" >> $GITHUB_OUTPUT
            exit 0  # Don't fail the workflow, just mark as inconclusive
          fi
      
      - name: Send Email on Failure
        if: failure()
        uses: dawidd6/action-send-mail@v3
        continue-on-error: true
        with:
          server_address: smtp.gmail.com
          server_port: 465
          secure: true
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: "‚ùå MTG AI Assistant - Nightly Jobs FAILED"
          to: ${{ secrets.ADMIN_EMAIL }}
          from: "MTG AI Assistant <${{ secrets.EMAIL_USERNAME }}>"
          body: |
            üö® NIGHTLY BULK DATA IMPORT FAILED üö®
            
            Run Date: ${{ github.event.repository.updated_at }}
            Workflow Run: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
            
            Job Results:
            - Job 2 (Price Import): ${{ steps.job2.outputs.job2_status || 'FAILED' }}
            - Job 2 Verification: ${{ steps.verify_job2.outputs.verify_job2_status || 'FAILED' }}
            - Job 3 (Price Snapshots): ${{ steps.job3.outputs.job3_status || 'FAILED' }}
            - Job 3 Verification: ${{ steps.verify_job3.outputs.verify_job3_status || 'FAILED' }}
            
            Please check the logs at the workflow link above.
            
            --- 
            Automated message from GitHub Actions
      
      - name: Summary
        if: always()
        run: |
          echo ""
          echo "üéâ =========================================="
          echo "üéâ NIGHTLY IMPORT SUMMARY"
          echo "üéâ =========================================="
          echo ""
          echo "üìÖ Run completed: $(date -u)"
          echo ""
          echo "Job Results:"
          echo "  üé® Job 1 (Metadata):  ${{ steps.job1.outputs.job1_status || 'DISABLED' }} - ${{ steps.job1.outputs.job1_duration || 'N/A' }}s"
          echo "  üí∞ Job 2 (Prices):    ${{ steps.job2.outputs.job2_status || 'FAILED' }} - ${{ steps.job2.outputs.job2_duration || 'N/A' }}s"
          echo "  üîç Job 2 Verification: ${{ steps.verify_job2.outputs.verify_job2_status || 'FAILED' }}"
          echo "  üìà Job 3 (Snapshots): ${{ steps.job3.outputs.job3_status || 'FAILED' }} - ${{ steps.job3.outputs.job3_duration || 'N/A' }}s"
          echo "  üîç Job 3 Verification: ${{ steps.verify_job3.outputs.verify_job3_status || 'FAILED' }}"
          echo ""
          
          TOTAL_TIME=$((
            ${{ steps.job1.outputs.job1_duration || 0 }} +
            ${{ steps.job2.outputs.job2_duration || 0 }} +
            ${{ steps.job3.outputs.job3_duration || 0 }}
          ))
          echo "‚è±Ô∏è Total runtime: ${TOTAL_TIME} seconds (~$((TOTAL_TIME / 60)) minutes)"
          echo ""
          
          if [ "${{ job.status }}" = "success" ]; then
            echo "‚úÖ All jobs completed successfully!"
          else
            echo "‚ùå Some jobs failed - check logs above"
          fi
      
      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: failure-logs-${{ github.run_id }}
          path: |
            response.json
          retention-days: 7

